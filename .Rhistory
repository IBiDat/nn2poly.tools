library(devtools)
use_mit_license("Pablo Morala")
use_vignette("my_vignette")
use_package("keras")
use_package("tensorflow")
use_package("magrittr")
document()
check()
check()
check()
library(devtools)
check()
library(nn2prPKG)
View(r)
r
use_package("nn2prPKG")
use_package("gtools")
use_package("stringi")
use_package("mvtnorm")
check()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(nn2prtools)
library(nn2prPKG)
library(tensorflow) #PONERESTO EN EL PAQUET# SACAR LA FUNCION DE SCALE? Y GENERATE?
library(keras)
# Fixed Parameters for the data generation
my_seed <- 12345
n_sample <- 500
mean_range <- c(-100, 100)
beta_range <- c(-50, 50)
error_var <- 0.2
p <- 3
q_original <- 3
# Set random seed for reproducibility (only affects R, not keras?)
set.seed(my_seed)
# Data generation:
data_generated <- generate_normal_data(n_sample, p, q_original, mean_range, beta_range, error_var)
data <- data_generated$data
original_betas <- data_generated$original_betas
# Scale the data in the desired interval and separate train and test
scale_method <- "-1,1"
data_scaled <- scale_data(data, scale_method)
aux <- divide_train_test(data_scaled, train_proportion = 0.75)
train <- aux$train
test <- aux$test
# Obtain parameters:
p <- ncol(train) - 1
# Divide train and test in x and y, keras needs them separated
x_train <- as.matrix(train[, 1:p])
y_train <- as.matrix(train[, p + 1])
x_test <- as.matrix(test[, 1:p])
y_test <- as.matrix(test[, p + 1])
# keras hyperparameters
my_loss <- "mse"
my_metrics <- "mse"
my_optimizer <- optimizer_rmsprop()
my_epochs <- 100
my_validation_split <- 0.2
my_verbose <- 1
# Parameters:
af_string_list <- list("softplus", "linear")
q_taylor_vector <- c(3, 1) # If the output is linear, the last value should be 1 so it doesnt affect the product
h_neurons_vector <- c(40, 1) # If the output is linear, the last value should be 1
my_max_norm <- list("l2_norm",1)
nn <- build_keras_model(
x_train,
y_train,
af_string_list,
h_neurons_vector,
my_max_norm,
my_loss,
my_optimizer,
my_metrics,
my_verbose,
my_epochs,
my_validation_split
)
x
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(nn2prtools)
library(nn2prPKG)
library(keras)
# Fixed Parameters for the data generation
my_seed <- 12345
n_sample <- 500
mean_range <- c(-100, 100)
beta_range <- c(-50, 50)
error_var <- 0.2
p <- 3
q_original <- 3
# Set random seed for reproducibility (only affects R, not keras?)
set.seed(my_seed)
# Data generation:
data_generated <- generate_normal_data(n_sample, p, q_original, mean_range, beta_range, error_var)
data <- data_generated$data
original_betas <- data_generated$original_betas
# Scale the data in the desired interval and separate train and test
scale_method <- "-1,1"
data_scaled <- scale_data(data, scale_method)
aux <- divide_train_test(data_scaled, train_proportion = 0.75)
train <- aux$train
test <- aux$test
# Obtain parameters:
p <- ncol(train) - 1
# Divide train and test in x and y, keras needs them separated
x_train <- as.matrix(train[, 1:p])
y_train <- as.matrix(train[, p + 1])
x_test <- as.matrix(test[, 1:p])
y_test <- as.matrix(test[, p + 1])
# keras hyperparameters
my_loss <- "mse"
my_metrics <- "mse"
my_optimizer <- optimizer_rmsprop()
my_epochs <- 100
my_validation_split <- 0.2
my_verbose <- 0
# Parameters:
af_string_list <- list("softplus", "linear")
q_taylor_vector <- c(3, 1) # If the output is linear, the last value should be 1 so it doesnt affect the product
h_neurons_vector <- c(40, 1) # If the output is linear, the last value should be 1
my_max_norm <- list("l2_norm",1)
nn <- build_keras_model(
x_train,
y_train,
af_string_list,
h_neurons_vector,
my_max_norm,
my_loss,
my_optimizer,
my_metrics,
my_verbose,
my_epochs,
my_validation_split
)
plot(nn)
document()
check()
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(nn2prtools)
library(nn2prtools)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(nn2prtools)
library(nn2prPKG)
library(keras)
# Fixed Parameters for the data generation
my_seed <- 12345
n_sample <- 500
mean_range <- c(-100, 100)
beta_range <- c(-50, 50)
error_var <- 0.2
p <- 3
q_original <- 3
# Set random seed for reproducibility (only affects R, not keras?)
set.seed(my_seed)
# Data generation:
data_generated <- generate_normal_data(n_sample, p, q_original, mean_range, beta_range, error_var)
data <- data_generated$data
original_betas <- data_generated$original_betas
# Scale the data in the desired interval and separate train and test
scale_method <- "-1,1"
data_scaled <- scale_data(data, scale_method)
aux <- divide_train_test(data_scaled, train_proportion = 0.75)
train <- aux$train
test <- aux$test
# Obtain parameters:
p <- ncol(train) - 1
# Divide train and test in x and y, keras needs them separated
x_train <- as.matrix(train[, 1:p])
y_train <- as.matrix(train[, p + 1])
x_test <- as.matrix(test[, 1:p])
y_test <- as.matrix(test[, p + 1])
# keras hyperparameters
my_loss <- "mse"
my_metrics <- "mse"
my_optimizer <- optimizer_rmsprop()
my_epochs <- 100
my_validation_split <- 0.2
my_verbose <- 0
# Parameters:
af_string_list <- list("softplus", "linear")
q_taylor_vector <- c(3, 1) # If the output is linear, the last value should be 1 so it doesnt affect the product
h_neurons_vector <- c(40, 1) # If the output is linear, the last value should be 1
my_max_norm <- list("l2_norm",1)
p <- ncol(x_train)
nn <- build_keras_model(p,
af_string_list,
h_neurons_vector,
my_max_norm)
# Compile the model
keras::compile(nn,
loss = my_loss,
optimizer = my_optimizer,
metrics = my_metrics
)
# Fit the model
history <- keras::fit(nn,
x_train,
y_train,
verbose = my_verbose,
epochs = my_epochs,
validation_split = my_validation_split,
batch_size = 500
)
# Fit the model
history <- keras::fit(nn,
x_train,
y_train,
verbose = my_verbose,
epochs = my_epochs,
validation_split = my_validation_split,
batch_size = 500
)
library(tensorflow)
# Fit the model
history <- keras::fit(nn,
x_train,
y_train,
verbose = my_verbose,
epochs = my_epochs,
validation_split = my_validation_split,
batch_size = 500
)
# Fit the model
history <- keras::fit(nn,
x_train,
y_train,
verbose = my_verbose,
epochs = my_epochs,
validation_split = my_validation_split,
batch_size = 500
)
nn
# keras hyperparameters
my_loss <- "mse"
my_metrics <- "mse"
my_optimizer <- optimizer_rmsprop()
my_epochs <- 100
my_validation_split <- 0.2
my_verbose <- 0
# Parameters:
af_string_list <- list("softplus", "linear")
q_taylor_vector <- c(3, 1) # If the output is linear, the last value should be 1 so it doesnt affect the product
h_neurons_vector <- c(40, 1) # If the output is linear, the last value should be 1
my_max_norm <- list("l2_norm",1)
p <- ncol(x_train)
nn <- build_keras_model(p,
af_string_list,
h_neurons_vector,
my_max_norm)
# Compile the model
keras::compile(nn,
loss = my_loss,
optimizer = my_optimizer,
metrics = my_metrics
)
# Fit the model
history <- keras::fit(nn,
x_train,
y_train,
verbose = my_verbose,
epochs = my_epochs,
validation_split = my_validation_split,
batch_size = 500
)
check()
library(devtools)
check()
library(nn2prtools)
